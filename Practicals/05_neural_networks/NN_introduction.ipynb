{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-15T15:38:16.424893Z",
     "start_time": "2025-04-15T15:38:16.422273Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Intro to tensors",
   "id": "6174048944299fb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:38:16.924297Z",
     "start_time": "2025-04-15T15:38:16.921345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Working with tensors\n",
    "a = torch.rand(10, 5)\n",
    "b = torch.rand(5, 17)\n",
    "mult = torch.matmul(a, b)\n",
    "print(mult.shape)"
   ],
   "id": "62fc21d202fa7d2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 17])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:38:17.187640Z",
     "start_time": "2025-04-15T15:38:17.184011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Working with tensors (batch multiplication)\n",
    "a = torch.rand(16, 10, 5)\n",
    "b = torch.rand(16, 5, 17)\n",
    "mult = torch.matmul(a, b)\n",
    "print(mult.shape)"
   ],
   "id": "b4d91e7fc3193251",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 17])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:49:00.657586Z",
     "start_time": "2025-04-15T15:49:00.650960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient example\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5, 6])\n",
    "c = torch.Tensor([7, 8, 9])\n",
    "\n",
    "a.requires_grad = True\n",
    "b.requires_grad = True\n",
    "c.requires_grad = True\n",
    "\n",
    "torch.sum((a * b) + c).backward()\n",
    "print(a.grad), print(b.grad), print(c.grad)"
   ],
   "id": "a8c2e65904c7aa67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:34:07.111978Z",
     "start_time": "2025-04-15T12:34:07.090350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.Tensor(torch.rand(1, 4))\n",
    "a.requires_grad = True\n",
    "b = a**2\n",
    "c = b*2\n",
    "d = c.mean()\n",
    "e = c.sum()"
   ],
   "id": "8919485bb99e2322",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T12:35:46.849237Z",
     "start_time": "2025-04-15T12:35:46.635194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d.backward(retain_graph=True) # fine\n",
    "e.backward(retain_graph=True) # fine\n",
    "d.backward() # also fine\n",
    "e.backward() # error will occur!"
   ],
   "id": "a35e7ab770ea873e",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m e\u001B[38;5;241m.\u001B[39mbackward(retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;66;03m# fine\u001B[39;00m\n\u001B[1;32m      3\u001B[0m d\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m# also fine\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# error will occur!\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/data-science-2/.venv/lib/python3.10/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/data-science-2/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/data-science-2/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "90c3259b136f7d2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:30:56.222990Z",
     "start_time": "2025-04-15T16:30:56.218991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create random dataset. Every Dataset has to implement __len__ and __getitem__\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, input_dim=20):\n",
    "        self.X = torch.rand(num_samples, input_dim)\n",
    "        self.y = (torch.mean(self.X, dim=1) > 1/2).type(torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "4aefc330c00b35ad",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:08:25.489464Z",
     "start_time": "2025-04-15T16:08:25.485077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create model\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dims=[64, 32], output_dim=2):\n",
    "        super(MultiLayerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.activation1 = nn.ReLU()\n",
    "            \n",
    "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.activation2 = nn.ReLU()\n",
    "            \n",
    "        self.head = nn.Linear(hidden_dims[1], output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ],
   "id": "22e248bec4c2cd2e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:56:51.014790Z",
     "start_time": "2025-04-15T15:56:51.011092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Train Loss: {avg_loss:.4f}\")"
   ],
   "id": "21f3cdcfe7b47869",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:56:53.023Z",
     "start_time": "2025-04-15T15:56:53.018704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Validation loop\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy"
   ],
   "id": "b11fa2308dcac0b9",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:59:19.621928Z",
     "start_time": "2025-04-15T15:59:19.617140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters and setup\n",
    "input_dim = 20\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and split\n",
    "dataset = SyntheticDataset(num_samples=1000, input_dim=input_dim)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "c48ed16bc6c071f2",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:59:21.228906Z",
     "start_time": "2025-04-15T15:59:19.947504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model, criterion, optimizer\n",
    "model = MultiLayerNet(input_dim=input_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ],
   "id": "80dd79a99a811b72",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:59:21.304234Z",
     "start_time": "2025-04-15T15:59:21.229922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full training + validation loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    validate(model, val_loader, criterion, device)"
   ],
   "id": "af86719ba29d45ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train Loss: 3.7927\n",
      "Validation Loss: 3.1987, Accuracy: 20.00%\n",
      "\n",
      "Epoch 2/5\n",
      "Train Loss: 3.6044\n",
      "Validation Loss: 3.2001, Accuracy: 19.00%\n",
      "\n",
      "Epoch 3/5\n",
      "Train Loss: 3.4255\n",
      "Validation Loss: 3.0971, Accuracy: 26.50%\n",
      "\n",
      "Epoch 4/5\n",
      "Train Loss: 3.2832\n",
      "Validation Loss: 3.0110, Accuracy: 28.00%\n",
      "\n",
      "Epoch 5/5\n",
      "Train Loss: 3.1521\n",
      "Validation Loss: 2.8996, Accuracy: 35.00%\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tasks\n",
    "- add early stopping\n",
    "- play with number of parameters in each layer, activation function and regularization parameters and observe how the training changes"
   ],
   "id": "7c7163f1683ce090"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e661b19e822874d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
