{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ],
   "id": "da3ace6b8049977f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning and bounding boxes\n",
    "- we take pre-trained model (e.g. ResNet) on image classification task\n",
    "- then we fine-tune the weights on bounding box task\n",
    "- we use Penn-Fudan database (see https://www.cis.upenn.edu/~jshi/ped_html/)\n",
    "- finally, we evaluate the model performance using standard metric\n",
    "- TODOs: 1) Implement ResNetBoxes, 2) Choose loss function, 3) Implement IoU as a metric and evaluate performance, 4) Improve ResNetBoxes with argument that determines if the backbone network should be freezed for fine-tuning, 5) Observe performance when different ResNet variants are used"
   ],
   "id": "42e3d1dadf6976e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try: # disable certificate verification, needed on MacOS\n",
    "    import ssl\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except ImportError:\n",
    "    pass  # SSL module not available, skipping workaround"
   ],
   "id": "45b0ad327819143c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download and extract dataset\n",
    "url = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\n",
    "download_and_extract_archive(url, download_root=\"data/\", extract_root=\"data/\", remove_finished=True)"
   ],
   "id": "f94699aa5cbabbe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class PennFudanDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.img_dir = os.path.join(root, \"PNGImages\")\n",
    "        self.mask_dir = os.path.join(root, \"PedMasks\")\n",
    "        self.imgs = sorted(os.listdir(self.img_dir))\n",
    "        self.masks = sorted(os.listdir(self.mask_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Construct full paths\n",
    "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        # Load image and mask\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "\n",
    "        # Remove background (assumed to be ID 0)\n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[obj_ids != 0]\n",
    "\n",
    "        # Generate binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # Generate bounding boxes\n",
    "        boxes = []\n",
    "        for m in masks:\n",
    "            pos = np.where(m)\n",
    "            xmin, xmax = pos[1].min(), pos[1].max()\n",
    "            ymin, ymax = pos[0].min(), pos[0].max()\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "        # All objects are labeled as class 1 (pedestrian)\n",
    "        labels = torch.ones((len(obj_ids),), dtype=torch.int64)\n",
    "\n",
    "        # Construct target dictionary\n",
    "        target = {\n",
    "            \"box\": boxes[0],  # First object's box only\n",
    "            \"label\": labels[0],\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ],
   "id": "79aef899acd25e12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResNetBoxes(nn.Module):\n",
    "    def __init__(self, resnet, freeze=False):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Get number of features from original ResNet fc layer\n",
    "        num_features = resnet.fc.in_features\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.a1 = nn.Flatten()\n",
    "        self.a2 = nn.Linear(num_features, 256)\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.a4 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.a4(x)\n",
    "        return x"
   ],
   "id": "972bf4cd222eb42a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "dataset = PennFudanDataset(root='data/PennFudanPed', transforms=transform)\n",
    "\n",
    "# Loader\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "backbone_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)  # download ResNet model\n",
    "model = ResNetBoxes(backbone_model, freeze=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.SmoothL1Loss()"
   ],
   "id": "1fcba23522273b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        gt_boxes = torch.stack([t for t in targets['box']]).to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, gt_boxes)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # TODO: add evaluation on test dataset using our 'compute_iou'\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(loader):.4f}\")"
   ],
   "id": "5a0e99d6ae766506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# IOU = area of overlap / area of union\n",
    "def compute_iou(box1, box2):\n",
    "    # box = [xmin, ymin, xmax, ymax]\n",
    "    ...\n",
    "\n",
    "# Evaluate after training\n",
    "model.eval()\n",
    "ious = []\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        gt_boxes = torch.stack([t for t in targets['box']]).to(device)\n",
    "        preds = model(imgs)\n",
    "\n",
    "        for pred, gt in zip(preds, gt_boxes):\n",
    "            iou = compute_iou(pred, gt)\n",
    "            ious.append(iou)\n",
    "\n",
    "print(f\"Mean IoU: {sum(ious) / len(ious):.4f}\")"
   ],
   "id": "766eb201dbfad239",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "56efdb57c74552ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
