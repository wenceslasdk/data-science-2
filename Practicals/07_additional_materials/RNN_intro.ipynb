{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - introduction\n",
    "\n",
    "In this tutorial we learn how the reccurent neural networks work and are used in text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from morpho_dataset import MorphoDataset\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morpho dataset\n",
    "- from Deep Learning course by Milan Straka\n",
    "- https://github.com/ufal/npfl114/tree/past-1920/labs/07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, pdt, args):\n",
    "        # Define a suitable model.\n",
    "\n",
    "        num_tags = len(pdt.train.data[pdt.train.TAGS].words)\n",
    "        num_words = len(pdt.train.data[pdt.train.FORMS].words)\n",
    "        num_chars = len(pdt.train.data[pdt.train.FORMS].alphabet)\n",
    "       \n",
    "        \n",
    "        # Implement a one-layer RNN network. The input\n",
    "        # `word_ids` consists of a batch of sentences, each\n",
    "        # a sequence of word indices. Padded words have index 0.\n",
    "\n",
    "        # Embed input words with dimensionality `args.we_dim`,\n",
    "        # using `mask_zero=True`.\n",
    "\n",
    "        word_ids = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "        we = tf.keras.layers.Embedding(input_dim=num_words, output_dim=args.we_dim, mask_zero=True)(word_ids)\n",
    "\n",
    "        # The RNN character-level embeddings utilize the input `charseqs`\n",
    "        # containing a sequence of character indices for every input word.\n",
    "        # Again, padded characters have index 0.\n",
    "\n",
    "        charseqs = tf.keras.Input(shape=(None, None, ), dtype='int32')\n",
    "\n",
    "        # Because cuDNN implementation of RNN does not allow empty sequences,\n",
    "        # we need to consider only charseqs for valid words.\n",
    "        valid_words = tf.where(word_ids != 0)\n",
    "        cle = tf.gather_nd(charseqs, valid_words)\n",
    "\n",
    "        # Embed the characters in `cle` using embeddings of size\n",
    "        # `args.cle_dim`, masking zero indices. Then, pass the embedded characters\n",
    "        # through a bidirectional GRU with dimension `args.cle_dim`, concatenating\n",
    "        # results from forward and backward pass. Store the computed embeddings\n",
    "        # in `cle` variable.\n",
    "\n",
    "        cle = tf.keras.layers.Embedding(input_dim=num_chars, output_dim=args.cle_dim, mask_zero=True)(cle)\n",
    "\n",
    "        forward_layer = tf.keras.layers.GRU(args.cle_dim)\n",
    "        backward_layer = tf.keras.layers.GRU(args.cle_dim, go_backwards=True)\n",
    "\n",
    "        cle = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer)(cle)\n",
    "\n",
    "        # Now we copy cle-s back to the original shape.\n",
    "        cle = tf.scatter_nd(valid_words, cle, [tf.shape(charseqs)[0], tf.shape(charseqs)[1], cle.shape[-1]])\n",
    "\n",
    "        # Concatenate the WE and CLE embeddings (in this order).\n",
    "        # Use a `tf.keras.layers.Concatenate()` layer, which preserves masks\n",
    "        # (contrary to raw methods like tf.concat).\n",
    "\n",
    "        concat = tf.keras.layers.Concatenate()([we, cle])\n",
    "\n",
    "        # Create specified `args.rnn_cell` RNN cell (LSTM, GRU) with\n",
    "        # dimension `args.rnn_cell_dim` and apply it in a bidirectional way on\n",
    "        # the embedded words, summing the outputs of forward and backward RNNs.\n",
    "\n",
    "        if args.rnn_cell == 'LSTM':\n",
    "            forward_layer = tf.keras.layers.LSTM(args.rnn_cell_dim, return_sequences=True)\n",
    "            backward_layer = tf.keras.layers.LSTM(args.rnn_cell_dim, return_sequences=True, go_backwards=True)\n",
    "        else:\n",
    "            forward_layer = tf.keras.layers.GRU(args.rnn_cell_dim, return_sequences=True)\n",
    "            backward_layer = tf.keras.layers.GRU(args.rnn_cell_dim, return_sequences=True, go_backwards=True)\n",
    "        rnn_layer = tf.keras.layers.Bidirectional(forward_layer, backward_layer=backward_layer, merge_mode='sum')(concat)\n",
    "\n",
    "\n",
    "        # Add a softmax classification layer into `num_tags` classes, storing\n",
    "        # the outputs in `predictions`.\n",
    "\n",
    "        predictions = tf.keras.layers.Dense(num_tags, activation=tf.nn.softmax)(rnn_layer)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=[word_ids, charseqs], outputs=predictions)\n",
    "        self.model.compile(optimizer=tf.optimizers.Adam(),\n",
    "                           loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "                           metrics=[tf.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
    "\n",
    "        self._writer = tf.summary.create_file_writer(args.logdir, flush_millis=10 * 1000)\n",
    "\n",
    "\n",
    "\n",
    "    def train_epoch(self, dataset, args):\n",
    "        for batch in dataset.batches(args.batch_size):\n",
    "            metrics = self.model.train_on_batch(\n",
    "                [batch[dataset.FORMS].word_ids, batch[dataset.FORMS].charseqs],\n",
    "                batch[dataset.TAGS].word_ids,\n",
    "                reset_metrics=True)\n",
    "\n",
    "            # Generate the summaries each 100 steps\n",
    "            if self.model.optimizer.iterations % 100 == 0:\n",
    "                tf.summary.experimental.set_step(self.model.optimizer.iterations)\n",
    "                with self._writer.as_default():\n",
    "                    for name, value in zip(self.model.metrics_names, metrics):\n",
    "                        tf.summary.scalar(\"train/{}\".format(name), value)\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, dataset, dataset_name, args):\n",
    "        # We assume that model metric are already resetted at this point.\n",
    "        for batch in dataset.batches(args.batch_size):\n",
    "            # Evaluate the given batch with `test_on_batch`, using the\n",
    "            # same inputs as in training, but pass `reset_metrics=False` to\n",
    "            # aggregate the metrics. Store the metrics of the last batch as `metrics`.\n",
    "            metrics = self.model.test_on_batch(\n",
    "                [batch[dataset.FORMS].word_ids, batch[dataset.FORMS].charseqs],\n",
    "                batch[dataset.TAGS].word_ids,\n",
    "                reset_metrics=False)\n",
    "        self.model.reset_metrics()\n",
    "\n",
    "        metrics = dict(zip(self.model.metrics_names, metrics))\n",
    "        with self._writer.as_default():\n",
    "            tf.summary.experimental.set_step(self.model.optimizer.iterations)\n",
    "            for name, value in metrics.items():\n",
    "                tf.summary.scalar(\"{}/{}\".format(dataset_name, name), value)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Define reasonable defaults and optionally more parameters\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--epochs\", default=100, type=int, help=\"Number of epochs.\")\n",
    "parser.add_argument(\"--we_dim\", default=2, type=int, help=\"Word embedding dimension.\")\n",
    "parser.add_argument(\"--rnn_cell\", default=\"LSTM\", type=str, help=\"RNN cell type.\")\n",
    "parser.add_argument(\"--rnn_cell_dim\", default=16, type=int, help=\"RNN cell dimension.\")\n",
    "parser.add_argument(\"--cle_dim\", default=16, type=int, help=\"CLE embedding dimension.\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\n",
    "parser.add_argument(\"--threads\", default=8, type=int, help=\"Maximum number of threads to use.\")\n",
    "parser.add_argument(\"--verbose\", default=False, action=\"store_true\", help=\"Verbose TF logging.\")\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seed and set threads\n",
    "np.random.seed(args.seed)\n",
    "tf.random.set_seed(args.seed)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(args.threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(args.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report only errors by default\n",
    "if not args.verbose:\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "morpho = MorphoDataset(\"czech_pdt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logdir name\n",
    "args.logdir = os.path.join(\"logs\", \"{}-{}-{}\".format(\n",
    "    os.path.basename(globals().get(\"__file__\", \"notebook\")),\n",
    "    datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
    "    \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", key), value) for key, value in sorted(vars(args).items())))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(morpho.train.data[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(morpho.train.data[0].words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(morpho.train.data[0].alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(morpho.train.data[0].alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho.train.data[0].word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(morpho.train.data[0].word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# targets\n",
    "morpho.train.data[morpho.train.TAGS].word_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho.train.data[morpho.train.TAGS].word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho.train.data[morpho.train.LEMMAS].word_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(morpho.train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morpho.train.data[0].words_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "morpho.train.data[0].alphabet_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho.train.data[0].charseqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(morpho, args)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: To make the following line work you need to install graphviz (if you have not done so in one of the previous classes)\n",
    "# 1) follow the instructions https://graphviz.gitlab.io/download/?fbclid=IwAR1V-lrRhho5rSfBVYXYISsighqRwOCOgMHLmL_DclkQrPtMXQaKj3mFcqs\n",
    "# 2) this notebook has been tested with version 8.0.3\n",
    "# 3) make sure you add it to the PATH variable (you are specifically asked during the installation) at least for local user\n",
    "\n",
    "tf.keras.utils.plot_model(network.model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(1):\n",
    "    network.train_epoch(morpho.train, args)\n",
    "    metrics = network.evaluate(morpho.dev, \"dev\", args)\n",
    "    print(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one can change the learning rate manually\n",
    "network.model.compile(optimizer=tf.optimizers.Adam(learning_rate= 0.0001),\n",
    "                       loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "                       metrics=[tf.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then perform more training\n",
    "for epoch in range(1):\n",
    "    network.train_epoch(morpho.train, args)\n",
    "    metrics = network.evaluate(morpho.dev, \"dev\", args)\n",
    "    print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TO DO:** Large Movie Review Dataset</span>\n",
    "\n",
    "- Download the data: https://ai.stanford.edu/%7Eamaas/data/sentiment/, unpack it into the Data folder\n",
    "- Use RNN to predict the sentiment of the review\n",
    "- use https://www.tensorflow.org/tutorials/keras/text_classification for inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unlabeled data\n",
    "\n",
    "from pathlib import Path\n",
    "remove_dir = Path('./../Data/aclImdb/train') / 'unsup'\n",
    "\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use this code to read the data\n",
    "# set batch_size and seed\n",
    "batch_size = 64\n",
    "seed = 42\n",
    "\n",
    "train_dir = Path('./../Data/aclImdb/train')\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir, \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2, \n",
    "    subset='training', \n",
    "    seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check a few examples\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
