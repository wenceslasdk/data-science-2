{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Convolutions\n",
    "- we inspect some basic convolutions"
   ],
   "id": "12d6dfebc8b804de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load image\n",
    "img = mpimg.imread('../Data/eiffel.jpg')\n",
    "\n",
    "# Keep only RGB channels\n",
    "img = img[:, :, :3]\n",
    "\n",
    "# Convert to torch tensor and move channels to first dimension\n",
    "img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)  # [C, H, W]\n",
    "\n",
    "print(f'Image shape: {img_tensor.shape}')  # Should be [3, H, W]\n",
    "\n",
    "# Convert to grayscale using the given formula\n",
    "# The weights are: Red: 0.3, Green: 0.59, Blue: 0.11\n",
    "weights = torch.tensor([0.3, 0.59, 0.11]).view(3, 1, 1)\n",
    "img_gray_tensor = (img_tensor * weights).sum(dim=0)  # [H, W]\n",
    "\n",
    "# Convert to numpy for display\n",
    "img_gray = img_gray_tensor.numpy()\n",
    "\n",
    "# Plot original image\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Plot grayscale image\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.imshow(img_gray, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "342f79f132e75c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def detect_borders(img_gray, mask, threshold=0.001):\n",
    "    \"\"\"\n",
    "    img_gray: torch.Tensor of shape [H, W], grayscale image\n",
    "    mask: torch.Tensor of shape [k, k], convolution kernel\n",
    "    threshold: float, threshold to binarize the convolved result\n",
    "    \"\"\"\n",
    "    # Ensure inputs are float tensors\n",
    "    img_tensor = img_gray.unsqueeze(0).unsqueeze(0).float()  # [1, 1, H, W]\n",
    "    kernel = mask.unsqueeze(0).unsqueeze(0).float()          # [1, 1, k, k]\n",
    "    \n",
    "    # Apply 2D convolution (no padding, no stride)\n",
    "    img_processed = F.conv2d(img_tensor, kernel)\n",
    "    img_processed = img_processed.squeeze(0).squeeze(0)  # [H_out, W_out]\n",
    "\n",
    "    # Display the convolved image\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.imshow(img_processed.detach().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Threshold and invert\n",
    "    img_thresholded = 1 - (img_processed > threshold).float()\n",
    "\n",
    "    # Display the thresholded image\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.imshow(img_thresholded.detach().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "id": "af24c835af654c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the vertical Sobel filter as a PyTorch tensor (https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "mask = torch.tensor([\n",
    "    [-1.,  0.,  1.],\n",
    "    [-2.,  0.,  2.],\n",
    "    [-1.,  0.,  1.]\n",
    "])\n",
    "\n",
    "detect_borders(img_gray_tensor, mask, threshold=0.25)"
   ],
   "id": "45942505f37f7d3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "detect_borders(img_gray_tensor, mask.transpose(0, 1), threshold=0.25)",
   "id": "93a7883aa61a9d7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Laplace edge detector, should do both horizontal and vertical\n",
    "mask = torch.tensor([\n",
    "    [0,  1,  0],\n",
    "    [1,  -4, 1],\n",
    "    [0,  1,  0]\n",
    "])\n",
    "detect_borders(img_gray_tensor, mask, threshold=0.25)"
   ],
   "id": "f56b08e046c5a486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# now with negative sign\n",
    "mask = torch.tensor([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "detect_borders(img_gray_tensor, mask, threshold=0.3)"
   ],
   "id": "fbf832d3e2e54aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify changes around a point\n",
    "mask = torch.tensor([\n",
    "    [0,   0,  -1,   0,   0],\n",
    "    [0,  -1,  -2,  -1,   0],\n",
    "    [-1, -2,  16,  -2,  -1],\n",
    "    [0,  -1,  -2,  -1,   0],\n",
    "    [0,   0,  -1,   0,   0]\n",
    "])\n",
    "\n",
    "detect_borders(img_gray_tensor, mask, threshold=0.3)"
   ],
   "id": "4668822f7b24ed0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create dataset and dataloader\n",
    "- we use MNIST \n",
    "- the dataset is downloaded (if not already present)\n",
    "- data augmentation is included\n",
    "- on MacOS some may encounter problem with certification when downloading the data, you can 'conda install -c conda-forge certifi' in your PyCharm Terminal and then"
   ],
   "id": "f7e4c1d7e3afcacb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try: # disable certificate verification, needed on MacOS\n",
    "    import ssl\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "except ImportError:\n",
    "    pass  # SSL module not available, skipping workaround"
   ],
   "id": "3a66a0045a489c69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define data augmentation and normalization for CIFAR-10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),  # Resize within CIFAR-10's native size\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),  # CIFAR-10 dataset mean (RGB)\n",
    "                         std=(0.2023, 0.1994, 0.2010))   # CIFAR-10 dataset std (RGB)\n",
    "])"
   ],
   "id": "4769570fa95d70bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)"
   ],
   "id": "bad6d6d55741626c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the 11th image and its label\n",
    "image, label = train_dataset[8]  # image is a tensor (3, 32, 32)\n",
    "print(f'Image with label {label}.')\n",
    "\n",
    "# Undo normalization for visualization\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
    "std = torch.tensor([0.2023, 0.1994, 0.2010])\n",
    "image = image * std[:, None, None] + mean[:, None, None]  # De-normalize\n",
    "\n",
    "# Convert the tensor from (C, H, W) to (H, W, C)\n",
    "image_tensor = image.permute(1, 2, 0)  # (H, W, C)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_tensor.numpy())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "id": "bfccfed539679f1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Train model\n",
    "\n",
    "- define a model as a CNN class (see below), define your own architecture using CNNs\n",
    "- use regularization, optimizer, loss,... of your choice and define hyperparameters so that you obtain high accuracy"
   ],
   "id": "84faae9fb9899fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # First convolution block\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Second convolution block\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(32 * 8 * 8, 10)  # Updated for 32x32 input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate and print summary\n",
    "model = CNNModel()\n",
    "print(model)\n",
    "\n",
    "# Optional: Summary with torchsummary\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "    summary(model, input_size=(3, 32, 32))  # <- Corrected input size\n",
    "except ImportError:\n",
    "    print(\"Install torchsummary with `pip install torchsummary` to see detailed summary.\")"
   ],
   "id": "1d63e5b62643553e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BiggerCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiggerCNN, self).__init__()\n",
    "\n",
    "        # First convolution block\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 32x32 -> 16x16\n",
    "\n",
    "        # Second convolution block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 16x16 -> 8x8\n",
    "\n",
    "        # Dropout + FC\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(64 * 8 * 8, 10)  # Updated for 32x32 input\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate and print model\n",
    "model_bigger = BiggerCNN()\n",
    "print(model_bigger)\n",
    "\n",
    "# Optional: View model summary\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "    summary(model_bigger, input_size=(3, 32, 32))  # Updated input size for CIFAR-10\n",
    "except ImportError:\n",
    "    print(\"Install torchsummary with `pip install torchsummary` to see detailed summary.\")"
   ],
   "id": "5daed1fd17bb89f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # First block: input (3, 32, 32) → output (16, 8, 8)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)  # 32→16\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 16→8\n",
    "\n",
    "        # Second block: (16, 8, 8) → (32, 4, 4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 8→4\n",
    "\n",
    "        # Third block: (32, 4, 4) → (64, 2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 4→2\n",
    "\n",
    "        # Fourth block: (64, 2, 2) → (128, 2, 2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Global average pooling + final layers\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ],
   "id": "27b18fbee7c0752d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"epochs\": 2,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"weight_decay\": 0.0001,\n",
    "}\n",
    "\n",
    "# Initialize WandB\n",
    "try: # disable certificate verification, needed on MacOS\n",
    "    import wandb\n",
    "    wandb.init(project=\"cifar10-cnn\", config=config)\n",
    "    wandb_working = True\n",
    "except ImportError:\n",
    "    wandb_working = False"
   ],
   "id": "c2ff52c5be75e2c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CustomCNN()\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])"
   ],
   "id": "3945597a2f568004",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "for epoch in range(config['epochs']):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Track progress within the epoch\n",
    "    print(f\"Epoch {epoch + 1}/{config['epochs']} - Training...\")\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs, labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print progress after every 100 batches (adjust as needed)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Average train loss and accuracy for the epoch\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"  Training Loss: {train_loss:.4f} - Training Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{config['epochs']} - Validation...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs, labels\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"  Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Log metrics to WandB\n",
    "    if wandb_working:\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"epoch\": epoch\n",
    "        })"
   ],
   "id": "5f2e6ed3719a4c56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final evaluation on the validation set\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs, labels\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_total += labels.size(0)\n",
    "        val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "print(f\"Final validation loss: {val_loss:.4f}, Final validation accuracy: {val_accuracy:.2f}%\")"
   ],
   "id": "cc590b600e13782f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a3e9bf0c7a8525d7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
